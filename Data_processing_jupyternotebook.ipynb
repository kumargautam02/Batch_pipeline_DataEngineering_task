{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8oJZsHY5lG6D"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-26 23:06:48,592 - Data_Processing Script started\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import json\n",
        "import zipfile\n",
        "import pyspark\n",
        "import logging\n",
        "import pandas as pd\n",
        "from database_script import get_connection\n",
        "from sqlalchemy import create_engine\n",
        "from sqlalchemy import text\n",
        "from sqlalchemy.exc import SQLAlchemyError\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "# from selenium import webdriver\n",
        "# from selenium.webdriver import chrome\n",
        "# from selenium.webdriver.common.by import By\n",
        "# from selenium.webdriver.chrome.service import Service\n",
        "# from webdriver_manager.chrome import ChromeDriverManager\n",
        "# from selenium.webdriver.common.keys import Keys\n",
        "# from selenium.webdriver.chrome.options import Options\n",
        "from data_ingestion_script import *\n",
        "logging.basicConfig(format='%(asctime)s - %(message)s', level=logging.INFO)\n",
        "logger = logging.getLogger('Data_Processing')\n",
        "logger.info('Data_Processing Script started')\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-26 23:06:49,065 - Running the requirements.txt file\n",
            "2024-02-26 23:06:58,230 - requirements.txt runned successfully\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: asttokens==2.4.1 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 1)) (2.4.1)\n",
            "Requirement already satisfied: attrs==23.2.0 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 2)) (23.2.0)\n",
            "Requirement already satisfied: certifi==2024.2.2 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 3)) (2024.2.2)\n",
            "Requirement already satisfied: cffi==1.16.0 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 4)) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer==3.3.2 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 5)) (3.3.2)\n",
            "Requirement already satisfied: colorama==0.4.6 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 6)) (0.4.6)\n",
            "Requirement already satisfied: comm==0.2.1 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 7)) (0.2.1)\n",
            "Requirement already satisfied: debugpy==1.8.1 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 8)) (1.8.1)\n",
            "Requirement already satisfied: decorator==5.1.1 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 9)) (5.1.1)\n",
            "Requirement already satisfied: exceptiongroup==1.2.0 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 10)) (1.2.0)\n",
            "Requirement already satisfied: executing==2.0.1 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 11)) (2.0.1)\n",
            "Requirement already satisfied: greenlet==3.0.3 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 12)) (3.0.3)\n",
            "Requirement already satisfied: h11==0.14.0 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 13)) (0.14.0)\n",
            "Requirement already satisfied: idna==3.6 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 14)) (3.6)\n",
            "Requirement already satisfied: ipykernel==6.29.2 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 15)) (6.29.2)\n",
            "Requirement already satisfied: ipython==8.22.1 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 16)) (8.22.1)\n",
            "Requirement already satisfied: jedi==0.19.1 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 17)) (0.19.1)\n",
            "Requirement already satisfied: jupyter_client==8.6.0 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 18)) (8.6.0)\n",
            "Requirement already satisfied: jupyter_core==5.7.1 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 19)) (5.7.1)\n",
            "Requirement already satisfied: matplotlib-inline==0.1.6 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 20)) (0.1.6)\n",
            "Requirement already satisfied: nest-asyncio==1.6.0 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 21)) (1.6.0)\n",
            "Requirement already satisfied: numpy==1.26.4 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 22)) (1.26.4)\n",
            "Requirement already satisfied: outcome==1.3.0.post0 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 23)) (1.3.0.post0)\n",
            "Requirement already satisfied: packaging==23.2 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 24)) (23.2)\n",
            "Requirement already satisfied: pandas==2.2.1 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 25)) (2.2.1)\n",
            "Requirement already satisfied: parso==0.8.3 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 26)) (0.8.3)\n",
            "Requirement already satisfied: platformdirs==4.2.0 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 27)) (4.2.0)\n",
            "Requirement already satisfied: prompt-toolkit==3.0.43 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 28)) (3.0.43)\n",
            "Requirement already satisfied: psutil==5.9.8 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 29)) (5.9.8)\n",
            "Requirement already satisfied: pure-eval==0.2.2 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 30)) (0.2.2)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 31)) (0.10.9.7)\n",
            "Requirement already satisfied: pycparser==2.21 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 32)) (2.21)\n",
            "Requirement already satisfied: Pygments==2.17.2 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 33)) (2.17.2)\n",
            "Requirement already satisfied: PySocks==1.7.1 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 34)) (1.7.1)\n",
            "Requirement already satisfied: pyspark==3.5.0 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 35)) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil==2.8.2 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 36)) (2.8.2)\n",
            "Requirement already satisfied: python-dotenv==1.0.1 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 37)) (1.0.1)\n",
            "Requirement already satisfied: pytz==2024.1 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 38)) (2024.1)\n",
            "Requirement already satisfied: pywin32==306 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 39)) (306)\n",
            "Requirement already satisfied: pyzmq==25.1.2 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 40)) (25.1.2)\n",
            "Requirement already satisfied: requests==2.31.0 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 41)) (2.31.0)\n",
            "Requirement already satisfied: selenium==4.18.1 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 42)) (4.18.1)\n",
            "Requirement already satisfied: six==1.16.0 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 43)) (1.16.0)\n",
            "Requirement already satisfied: sniffio==1.3.0 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 44)) (1.3.0)\n",
            "Requirement already satisfied: sortedcontainers==2.4.0 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 45)) (2.4.0)\n",
            "Requirement already satisfied: SQLAlchemy==2.0.27 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 46)) (2.0.27)\n",
            "Requirement already satisfied: stack-data==0.6.3 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 47)) (0.6.3)\n",
            "Requirement already satisfied: tornado==6.4 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 48)) (6.4)\n",
            "Requirement already satisfied: traitlets==5.14.1 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 49)) (5.14.1)\n",
            "Requirement already satisfied: trio==0.24.0 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 50)) (0.24.0)\n",
            "Requirement already satisfied: trio-websocket==0.11.1 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 51)) (0.11.1)\n",
            "Requirement already satisfied: typing_extensions==4.10.0 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 52)) (4.10.0)\n",
            "Requirement already satisfied: tzdata==2024.1 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 53)) (2024.1)\n",
            "Requirement already satisfied: urllib3==2.2.1 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 54)) (2.2.1)\n",
            "Requirement already satisfied: wcwidth==0.2.13 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 55)) (0.2.13)\n",
            "Requirement already satisfied: webdriver-manager==4.0.1 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 56)) (4.0.1)\n",
            "Requirement already satisfied: wsproto==1.2.0 in c:\\users\\admin\\downloads\\procesing\\batch_pipeline_dataengineering_task\\.venv\\lib\\site-packages (from -r requirements.txt (line 57)) (1.2.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "logger.info(\"Running the requirements.txt file\")\n",
        "!pip install -r requirements.txt\n",
        "logger.info(\"requirements.txt runned successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-26 23:06:58,321 - ====== WebDriver manager ======\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-26 23:07:00,127 - Get LATEST chromedriver version for google-chrome\n",
            "2024-02-26 23:07:00,470 - Get LATEST chromedriver version for google-chrome\n",
            "2024-02-26 23:07:00,642 - Driver [C:\\Users\\Admin\\.wdm\\drivers\\chromedriver\\win64\\122.0.6261.69\\chromedriver-win32/chromedriver.exe] found in cache\n",
            "2024-02-26 23:07:18,582 - Download of Male_Dataset Started\n",
            "2024-02-26 23:07:21,046 - Download of Female_Dataset Started\n",
            "2024-02-26 23:07:23,091 - Ingestion of Data Completed successfully\n",
            "2024-02-26 23:07:23,096 - LANDING PATH IS THERE\n",
            "2024-02-26 23:07:23,099 - DOWNLOAD PATH IS THERE\n",
            "2024-02-26 23:07:23,607 - Data Loaded to Download path Successfully\n",
            "2024-02-26 23:07:23,610 - Started Extraction of file odis_female_json.zip\n",
            "2024-02-26 23:07:25,345 - Started Extraction of file odis_male_json.zip\n",
            "2024-02-26 23:07:34,685 - Data Extraction Completed Successfully\n"
          ]
        }
      ],
      "source": [
        "def start_ingesting_data(origin, target_directory):\n",
        "    try:\n",
        "        # Data Ingestion  to correct path\n",
        "        origin = origin\n",
        "        target_directory = target_directory\n",
        "        download_required_files(logger)\n",
        "        logger.info(\"Ingestion of Data Completed successfully\")\n",
        "\n",
        "        file_in_origin = os.listdir(origin)\n",
        "        \n",
        "        while (\"odis_female_json.zip\" not in file_in_origin) and (\"odis_male_json.zip\" not in file_in_origin):\n",
        "            file_in_origin = os.listdir(origin)\n",
        "            if (\"odis_female_json.zip\" in file_in_origin) and (\"odis_male_json.zip\" in file_in_origin):\n",
        "                logger.info(\"waiting for file to get downloaded\")\n",
        "                break\n",
        "        if ('LANDING' in os.listdir(f'{target_directory}')) and ('DOWNLOAD_PATH' in os.listdir(f'{target_directory}')):\n",
        "            logger.info(\"LANDING PATH IS THERE\")\n",
        "            logger.info(\"DOWNLOAD PATH IS THERE\")\n",
        "        else :\n",
        "            os.makedirs(f'{target_directory}LANDING')\n",
        "            logger.info(\"LANDING path created successfully\")\n",
        "            os.makedirs(f'{target_directory}DOWNLOAD_PATH')\n",
        "            logger.info(\"DOWNLOAD_PATH created created successfully\")\n",
        "\n",
        "        files_in_target_directory = os.listdir(target_directory+'DOWNLOAD_PATH')\n",
        "        needed_files = ['odis_female_json.zip', \"odis_male_json.zip\"]\n",
        "        file_in_origin = os.listdir(origin)\n",
        "        for file in file_in_origin:\n",
        "            if (file.startswith(\"odis_female_json\") or file.startswith(\"odis_male_json\")) and (('odis_female_json.json' not in files_in_target_directory) and ('odis_male_json.json' not in files_in_target_directory)):\n",
        "                shutil.copy(origin+file, target_directory+'DOWNLOAD_PATH')\n",
        "        logger.info(\"Data Loaded to Download path Successfully\")\n",
        "\n",
        "        #extracting all the files in landing folder\n",
        "        for file_name in needed_files:\n",
        "            if (file_name == \"odis_female_json.zip\") or (file_name == \"odis_male_json.zip\"):\n",
        "              logger.info(f\"Started Extraction of file {file_name}\")\n",
        "              with zipfile.ZipFile(f'{target_directory}/DOWNLOAD_PATH/{file_name}') as f:\n",
        "                      # f.extractall()/\n",
        "                      f.extractall(f'{target_directory}LANDING/')\n",
        "        logger.info(\"Data Extraction Completed Successfully\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        logging.error(\"Exception occurred\", exc_info=True)\n",
        "\n",
        "origin = 'C:/Users/Admin/Downloads/'\n",
        "target_directory = 'C:/Users/Admin/Downloads/procesing/Batch_pipeline_DataEngineering_task/'\n",
        "start_ingesting_data(origin, target_directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "K6COIFEtYaqB"
      },
      "outputs": [],
      "source": [
        "#Necessay Function\n",
        "def get_batter_bowler_striker(column, column_need):\n",
        "  try:\n",
        "    if column_need.strip() == 'batter':\n",
        "      return column['batter']\n",
        " \n",
        "    elif column_need.strip() == 'bowler':\n",
        "      return column['bowler']\n",
        "\n",
        "    elif column_need.strip() == 'non_striker':\n",
        "      return column['non_striker']\n",
        "  except Exception as e:\n",
        "    logging.error(\"Exception occurred\", exc_info=True)\n",
        "\n",
        "\n",
        "\n",
        "#Necessay Function\n",
        "def get_run_ball_by_ball(column):\n",
        "  try:\n",
        "\n",
        "    runs_list = [-1]*3\n",
        "    for scores in column.keys():\n",
        "      if scores == 'runs':\n",
        "        return column[scores]\n",
        "      \n",
        "  except Exception as e:\n",
        "    logging.error(\"Exception occurred\", exc_info=True)\n",
        "\n",
        "\n",
        "def get_info_and_meta_data(all_information):\n",
        "  try:\n",
        "      for data in all_information.keys():\n",
        "        if data == 'meta':\n",
        "          meta_information = all_information['meta']\n",
        "        elif data == 'info':\n",
        "          info_information = all_information[\"info\"]\n",
        "      return meta_information, info_information\n",
        "  except Exception as e:\n",
        "    logging.error(\"Exception occurred\", exc_info=True)\n",
        "\n",
        "def get_necessary_information(data):\n",
        "  try:\n",
        "      date = data['dates'][0]\n",
        "\n",
        "      if \"city\" in data.keys():\n",
        "        city = data['city']\n",
        "      else:\n",
        "        city = \"NULL\"\n",
        "      if 'event' in data['event'].keys():\n",
        "        event_name = data['event']['name']\n",
        "        if 'match_number' in data['event'].keys():\n",
        "          match_number = data['event']['match_number']\n",
        "        else:\n",
        "          match_number = \"NULL\"\n",
        "      else:\n",
        "        event_name = \"NULL\"\n",
        "        match_number = \"NULL\"\n",
        "\n",
        "      gender = data['gender']\n",
        "\n",
        "      if \"winner\" in data['outcome'].keys():\n",
        "\n",
        "        winner_team = data['outcome']['winner']\n",
        "      elif 'result' in data['outcome'].keys():\n",
        "\n",
        "        winner_team = data['outcome']['result']\n",
        "\n",
        "      if \"winner\" in data['outcome'].keys():\n",
        "        if 'wickets' in data['outcome']['by'].keys():\n",
        "          winned_by = f\"{data['outcome']['by']['wickets']} wickets\"\n",
        "        elif 'runs' in data['outcome']['by'].keys():\n",
        "          winned_by = f\"{data['outcome']['by']['runs']} runs\"\n",
        "      else:\n",
        "        winned_by = \"NULL\"\n",
        "      team_1 = data['teams'][0]\n",
        "      team_2 = data['teams'][1]\n",
        "      return date, city, event_name, match_number, gender, winner_team, winned_by, team_1, team_2\n",
        "  except Exception as e:\n",
        "    logging.error(\"Exception occurred\", exc_info=True)\n",
        "\n",
        "#UDF created to get the run scored ball-by-ball\n",
        "get_run_ball_by_ball_udf =  udf(lambda column: get_run_ball_by_ball(column), StringType())\n",
        "#UDF created to get the batter name, striker name, bowler name\n",
        "get_batter_bowler_striker_udf =  udf(lambda column, column_need: get_batter_bowler_striker(column, column_need), StringType())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8X_QVfXEAO8",
        "outputId": "d17df61c-d119-4cd8-97f2-abaf331a14a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+----------+-----------+----------+----------+----------+------------+------+-----------+---------+---------+---------------+-----------+---------------------------+-------------------+----------------------+\n",
            "|overs|FIRST_TEAM|SECOND_TEAM|EVENT_NAME|MATCH_DATE|MATCH_CITY|MATCH_NUMBER|GENDER|WINNER_TEAM|WINNED_BY|BATTER   |BOWLER         |NON_STRIKER|BATTER_SCORED_RUNS_PER_BALL|TOTAL_RUNS_PER_BALL|EXTRAS_EARNED_PER_BALL|\n",
            "+-----+----------+-----------+----------+----------+----------+------------+------+-----------+---------+---------+---------------+-----------+---------------------------+-------------------+----------------------+\n",
            "|0    |Australia |Pakistan   |NULL      |2017-01-13|Brisbane  |NULL        |male  |Australia  |92 runs  |DA Warner|Mohammad Amir  |TM Head    |0                          |extras=0           |{0                    |\n",
            "|0    |Australia |Pakistan   |NULL      |2017-01-13|Brisbane  |NULL        |male  |Australia  |92 runs  |DA Warner|Mohammad Amir  |TM Head    |0                          |extras=0           |{0                    |\n",
            "|0    |Australia |Pakistan   |NULL      |2017-01-13|Brisbane  |NULL        |male  |Australia  |92 runs  |DA Warner|Mohammad Amir  |TM Head    |0                          |extras=0           |{0                    |\n",
            "|0    |Australia |Pakistan   |NULL      |2017-01-13|Brisbane  |NULL        |male  |Australia  |92 runs  |DA Warner|Mohammad Amir  |TM Head    |0                          |extras=0           |{0                    |\n",
            "|0    |Australia |Pakistan   |NULL      |2017-01-13|Brisbane  |NULL        |male  |Australia  |92 runs  |DA Warner|Mohammad Amir  |TM Head    |1                          |extras=1           |{0                    |\n",
            "|0    |Australia |Pakistan   |NULL      |2017-01-13|Brisbane  |NULL        |male  |Australia  |92 runs  |DA Warner|Mohammad Amir  |TM Head    |0                          |extras=0           |{0                    |\n",
            "|0    |Australia |Pakistan   |NULL      |2017-01-13|Brisbane  |NULL        |male  |Australia  |92 runs  |DA Warner|Mohammad Amir  |TM Head    |0                          |extras=0           |{0                    |\n",
            "|1    |Australia |Pakistan   |NULL      |2017-01-13|Brisbane  |NULL        |male  |Australia  |92 runs  |TM Head  |Mohammad Hafeez|DA Warner  |0                          |extras=0           |{0                    |\n",
            "|1    |Australia |Pakistan   |NULL      |2017-01-13|Brisbane  |NULL        |male  |Australia  |92 runs  |TM Head  |Mohammad Hafeez|DA Warner  |1                          |extras=0           |{1                    |\n",
            "|1    |Australia |Pakistan   |NULL      |2017-01-13|Brisbane  |NULL        |male  |Australia  |92 runs  |DA Warner|Mohammad Hafeez|TM Head    |0                          |extras=0           |{0                    |\n",
            "+-----+----------+-----------+----------+----------+----------+------------+------+-----------+---------+---------+---------------+-----------+---------------------------+-------------------+----------------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "C:/Users/Admin/Downloads/procesing/Batch_pipeline_DataEngineering_task/LANDING//49\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  path = target_directory+'LANDING/'\n",
        "  all_the_files = os.listdir(path)\n",
        "  # print(all_the_files)\n",
        "  necessary_columns = []\n",
        "  for i in all_the_files:\n",
        "    # print(f\"/female_dataset/{i}\")\n",
        "    # print(i)\n",
        "    if i.endswith(\".json\"):\n",
        "      with open(f\"{path}/{i}\",\"r\") as file_obj:\n",
        "        file_content = file_obj.read()\n",
        "        # print(file_content)\n",
        "        details = json.loads(file_content)\n",
        "        meta_information, info_information = get_info_and_meta_data(details)\n",
        "        date, city, event_name, match_number, gender, winner_team, winned_by, team_1, team_2 = get_necessary_information(info_information)\n",
        "        necessary_columns = []\n",
        "        temp_dict = {}\n",
        "        for i in range(len(details['innings'][0]['overs'])):\n",
        "          temp_dict['overs'] = i\n",
        "          temp_dict['balls_per_over'] = details['innings'][0]['overs'][i]['deliveries']\n",
        "          # print(student_details['innings'][0]['overs'][i]['deliveries'])\n",
        "          necessary_columns.append(temp_dict)\n",
        "          temp_dict = {}\n",
        "        dataframe = spark.createDataFrame(necessary_columns)\n",
        "        # dataframe.show(1000, False)\n",
        "        dataframe = dataframe.select(\"overs\", posexplode_outer(dataframe.balls_per_over))\n",
        "\n",
        "        dataframe_new = dataframe.withColumn(\"BATTER\", get_batter_bowler_striker_udf(col(\"col\"), lit(\"batter\")))\n",
        "        dataframe_new = dataframe_new.withColumn(\"BOWLER\", get_batter_bowler_striker_udf(col(\"col\"), lit(\"bowler\")))\n",
        "        dataframe_new = dataframe_new.withColumn(\"NON_STRIKER\", get_batter_bowler_striker_udf(col(\"col\"), lit(\"non_striker\")))\n",
        "        dataframe_new = dataframe_new.withColumn(\"runs_scored_per_ball\", get_run_ball_by_ball_udf(col(\"col\")))\n",
        "        dataframe_new = dataframe_new.select('*', lit(date).alias(\"MATCH_DATE\"), lit(city).alias(\"MATCH_CITY\"),\\\n",
        "                                            lit(event_name).alias(\"EVENT_NAME\"),lit(match_number).alias(\"MATCH_NUMBER\"),lit(gender).alias(\"GENDER\"),\\\n",
        "                                            lit(winner_team).alias(\"WINNER_TEAM\"),lit(winned_by).alias(\"WINNED_BY\"),lit(team_1).alias(\"FIRST_TEAM\"),lit(team_2).alias(\"SECOND_TEAM\"))\n",
        "\n",
        "        dataframe_new = dataframe_new.withColumn(\"runs_scored_per_ball\", regexp_replace(col(\"runs_scored_per_ball\"), \"(\\{extras=)|(total=)|(batter=)|(\\})\", \"\")).withColumn(\"EXTRAS_EARNED_PER_BALL\", trim(split(col(\"runs_scored_per_ball\"), ',').getItem(0))).withColumn(\"TOTAL_RUNS_PER_BALL\", trim(split(col(\"runs_scored_per_ball\"), ',').getItem(1))).withColumn(\"BATTER_SCORED_RUNS_PER_BALL\", trim(split(col(\"runs_scored_per_ball\"), ',').getItem(2)))\n",
        "        dataframe_new = dataframe_new.select('overs','FIRST_TEAM', 'SECOND_TEAM','EVENT_NAME','MATCH_DATE','MATCH_CITY','MATCH_NUMBER','GENDER','WINNER_TEAM','WINNED_BY','BATTER','BOWLER','NON_STRIKER','BATTER_SCORED_RUNS_PER_BALL','TOTAL_RUNS_PER_BALL', 'EXTRAS_EARNED_PER_BALL')\n",
        "        # dataframe_new.write.parquet(\"/output_folder/\")\\\n",
        "        # dataframe_new.write.mode(\"append\").format(\"parquet\").save(\"/output_folder/\")\n",
        "        dataframe_new.show(10, False)\n",
        "        # dataframe_new = dataframe_new.toPandas()\n",
        "        # dataframe_new.to_sql(con = my_conn, name = 'male', if_exists='replace')\n",
        "        # dataframe_new.show(10, Fals\n",
        "        # result = my_conn.execute(text(\"select * from male where WINNED_BY NOT like '%runs%'\"))\n",
        "        # print(result.all())\n",
        "        print(f\"{path}/{i}\")\n",
        "        break\n",
        "except Exception as e:\n",
        "  logging.error(\"Exception occurred\", exc_info=True)\n",
        "   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "49\n"
          ]
        }
      ],
      "source": [
        "print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "get_connection() missing 2 required positional arguments: 'logger' and 'path_of_database'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m conn \u001b[38;5;241m=\u001b[39m \u001b[43mget_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(conn)\n\u001b[0;32m      3\u001b[0m create_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCREATE TABLE IF NOT EXISTS ODI_CRICKET_RESULT(overs INTEGER NOT NULL, FIRST_TEAM VARCHAR(30), SECOND_TEAM VARCHAR(30), EVENT_NAME VARCHAR(60), MATCH_DATE DATE , MATCH_CITY VARCHAR(50), MATCH_NUMBER INTEGER, GENDER VARCHAR(50), WINNER_TEAM VARCHAR(50), WINNED_BY VARCHAR(50), BATTER VARCHAR(50), BOWLER VARCHAR(50), NON_STRIKER VARCHAR(50), BATTER_SCORED_RUNS_PER_BALL VARCHAR(50), TOTAL_RUNS_PER_BALL VARCHAR(50), EXTRAS_EARNED_PER_BALL VARCHAR(50))\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "\u001b[1;31mTypeError\u001b[0m: get_connection() missing 2 required positional arguments: 'logger' and 'path_of_database'"
          ]
        }
      ],
      "source": [
        "conn = get_connection()\n",
        "print(conn)\n",
        "create_table = \"CREATE TABLE IF NOT EXISTS ODI_CRICKET_RESULT(overs INTEGER NOT NULL, FIRST_TEAM VARCHAR(30), SECOND_TEAM VARCHAR(30), EVENT_NAME VARCHAR(60), MATCH_DATE DATE , MATCH_CITY VARCHAR(50), MATCH_NUMBER INTEGER, GENDER VARCHAR(50), WINNER_TEAM VARCHAR(50), WINNED_BY VARCHAR(50), BATTER VARCHAR(50), BOWLER VARCHAR(50), NON_STRIKER VARCHAR(50), BATTER_SCORED_RUNS_PER_BALL VARCHAR(50), TOTAL_RUNS_PER_BALL VARCHAR(50), EXTRAS_EARNED_PER_BALL VARCHAR(50))\"\n",
        "# my_conn = my_conn.connect()\n",
        "# result = my_conn.execute(text(create_table))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = conn.execute(text(create_table))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ODI_CRICKET_RESULT\n"
          ]
        }
      ],
      "source": [
        "result = conn.execute(text(\"select name from sqlite_master\"))\n",
        "for tables in result.all():\n",
        "    if tables[0] != \"ODI_CRICKET_RESULT\":\n",
        "        create_table = \"CREATE TABLE IF NOT EXISTS ODI_CRICKET_RESULT(overs INTEGER NOT NULL, \\\n",
        "        FIRST_TEAM VARCHAR(30), SECOND_TEAM VARCHAR(30), EVENT_NAME VARCHAR(60), MATCH_DATE DATE , \\\n",
        "        MATCH_CITY VARCHAR(50), MATCH_NUMBER INTEGER, GENDER VARCHAR(50), WINNER_TEAM VARCHAR(50), WINNED_BY VARCHAR(50),\\\n",
        "        BATTER VARCHAR(50), BOWLER VARCHAR(50), NON_STRIKER VARCHAR(50), BATTER_SCORED_RUNS_PER_BALL VARCHAR(50), TOTAL_RUNS_PER_BALL VARCHAR(50), EXTRAS_EARNED_PER_BALL VARCHAR(50))\"\n",
        "        conn.execute(text(create_table))\n",
        "        logger.info(\"ODI_CRICKET_RESULT table created successfully\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "ResourceClosedError",
          "evalue": "This result object does not return rows. It has been closed automatically.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mResourceClosedError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[28], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# DROP TABLE table_name;\u001b[39;00m\n\u001b[0;32m      2\u001b[0m result \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mexecute(text(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDROP TABLE male\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mprint\u001b[39m(\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n",
            "File \u001b[1;32mc:\\Users\\Admin\\Downloads\\procesing\\Batch_pipeline_DataEngineering_task\\.venv\\lib\\site-packages\\sqlalchemy\\engine\\result.py:1379\u001b[0m, in \u001b[0;36mResult.all\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mall\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Sequence[Row[_TP]]:\n\u001b[0;32m   1363\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return all rows in a sequence.\u001b[39;00m\n\u001b[0;32m   1364\u001b[0m \n\u001b[0;32m   1365\u001b[0m \u001b[38;5;124;03m    Closes the result set after invocation.   Subsequent invocations\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1376\u001b[0m \n\u001b[0;32m   1377\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1379\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_allrows\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Admin\\Downloads\\procesing\\Batch_pipeline_DataEngineering_task\\.venv\\lib\\site-packages\\sqlalchemy\\engine\\result.py:546\u001b[0m, in \u001b[0;36mResultInternal._allrows\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_allrows\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[_R]:\n\u001b[0;32m    544\u001b[0m     post_creational_filter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_creational_filter\n\u001b[1;32m--> 546\u001b[0m     make_row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_row_getter\u001b[49m\n\u001b[0;32m    548\u001b[0m     rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetchall_impl()\n\u001b[0;32m    549\u001b[0m     made_rows: List[_InterimRowType[_R]]\n",
            "File \u001b[1;32mc:\\Users\\Admin\\Downloads\\procesing\\Batch_pipeline_DataEngineering_task\\.venv\\lib\\site-packages\\sqlalchemy\\util\\langhelpers.py:1252\u001b[0m, in \u001b[0;36mHasMemoized.memoized_attribute.__get__\u001b[1;34m(self, obj, cls)\u001b[0m\n\u001b[0;32m   1250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m-> 1252\u001b[0m obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m] \u001b[38;5;241m=\u001b[39m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1253\u001b[0m obj\u001b[38;5;241m.\u001b[39m_memoized_keys \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m}\n\u001b[0;32m   1254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "File \u001b[1;32mc:\\Users\\Admin\\Downloads\\procesing\\Batch_pipeline_DataEngineering_task\\.venv\\lib\\site-packages\\sqlalchemy\\engine\\result.py:471\u001b[0m, in \u001b[0;36mResultInternal._row_getter\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    467\u001b[0m     process_row \u001b[38;5;241m=\u001b[39m Row  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    469\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[1;32m--> 471\u001b[0m key_to_index \u001b[38;5;241m=\u001b[39m \u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_key_to_index\u001b[49m\n\u001b[0;32m    472\u001b[0m processors \u001b[38;5;241m=\u001b[39m metadata\u001b[38;5;241m.\u001b[39m_effective_processors\n\u001b[0;32m    473\u001b[0m tf \u001b[38;5;241m=\u001b[39m metadata\u001b[38;5;241m.\u001b[39m_tuplefilter\n",
            "File \u001b[1;32mc:\\Users\\Admin\\Downloads\\procesing\\Batch_pipeline_DataEngineering_task\\.venv\\lib\\site-packages\\sqlalchemy\\engine\\cursor.py:1358\u001b[0m, in \u001b[0;36m_NoResultMetaData._key_to_index\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1356\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m   1357\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_key_to_index\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m-> 1358\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_we_dont_return_rows\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Admin\\Downloads\\procesing\\Batch_pipeline_DataEngineering_task\\.venv\\lib\\site-packages\\sqlalchemy\\engine\\cursor.py:1338\u001b[0m, in \u001b[0;36m_NoResultMetaData._we_dont_return_rows\u001b[1;34m(self, err)\u001b[0m\n\u001b[0;32m   1337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_we_dont_return_rows\u001b[39m(\u001b[38;5;28mself\u001b[39m, err\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m-> 1338\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mResourceClosedError(\n\u001b[0;32m   1339\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis result object does not return rows. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1340\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt has been closed automatically.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1341\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
            "\u001b[1;31mResourceClosedError\u001b[0m: This result object does not return rows. It has been closed automatically."
          ]
        }
      ],
      "source": [
        "# DROP TABLE table_name;\n",
        "result = conn.execute(text(\"DROP TABLE male\"))\n",
        "# print(print(result.all()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Admin\\Downloads\\procesing\\Batch_pipeline_DataEngineering_task\n"
          ]
        }
      ],
      "source": [
        "working_directory = os.getcwd()\n",
        "print(working_directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'conn' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m conn \u001b[38;5;241m=\u001b[39m \u001b[43mget_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworking_directory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(conn)\n",
            "File \u001b[1;32mc:\\Users\\Admin\\Downloads\\procesing\\Batch_pipeline_DataEngineering_task\\database_script.py:10\u001b[0m, in \u001b[0;36mget_connection\u001b[1;34m(logger, path_of_database)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_connection\u001b[39m(logger, path_of_database):\n\u001b[0;32m      9\u001b[0m     path \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqlite:///C:/Users/Admin/Downloads/procesing/Batch_pipeline_DataEngineering_task/database/cricket.db\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241m.\u001b[39mexecute(text(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mselect name from sqlite_master\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tables \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m tables[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mODI_CRICKET_RESULT\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "\u001b[1;31mNameError\u001b[0m: name 'conn' is not defined"
          ]
        }
      ],
      "source": [
        "conn = get_connection(logger, working_directory)\n",
        "print(conn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "and (('odis_female_json.json' not in files_in_target_directory) and ('odis_male_json.json' not in files_in_target_directory))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "path = (\"sqlite:///c:/Users/Admin/Downloads/procesing/Batch_pipeline_DataEngineering_task/DATABASE/cricket.db\")\n",
        "my_conn = create_engine(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'Engine' object has no attribute 'execute'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmy_conn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m(text(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mselect name from sqlite_master\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(result\u001b[38;5;241m.\u001b[39mall())\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'Engine' object has no attribute 'execute'"
          ]
        }
      ],
      "source": [
        "result = my_conn.execute(text(\"select name from sqlite_master\"))\n",
        "print(result.all())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
